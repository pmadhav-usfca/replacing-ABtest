{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cdbca743-ecde-4b10-9e4d-79d34146a88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from scipy import stats\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression,Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, accuracy_score,mean_squared_error\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ignore all warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7131c7c0-5358-452f-aaf8-3f23f89b4fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>clickability_test_id</th>\n",
       "      <th>headline</th>\n",
       "      <th>impressions</th>\n",
       "      <th>clicks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-11-20 11:33:26.475</td>\n",
       "      <td>546dd17e26714c82cc00001c</td>\n",
       "      <td>Let’s See … Hire Cops, Pay Teachers, Buy Books...</td>\n",
       "      <td>3118</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-11-20 15:00:01.032</td>\n",
       "      <td>546e01d626714c6c4400004e</td>\n",
       "      <td>People Sent This Lesbian Questions And Her Rai...</td>\n",
       "      <td>4587</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-11-20 11:33:51.973</td>\n",
       "      <td>546dd17e26714c82cc00001c</td>\n",
       "      <td>$3 Million Is What It Takes For A State To Leg...</td>\n",
       "      <td>3017</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                created_at      clickability_test_id  \\\n",
       "0  2014-11-20 11:33:26.475  546dd17e26714c82cc00001c   \n",
       "1  2014-11-20 15:00:01.032  546e01d626714c6c4400004e   \n",
       "2  2014-11-20 11:33:51.973  546dd17e26714c82cc00001c   \n",
       "\n",
       "                                            headline  impressions  clicks  \n",
       "0  Let’s See … Hire Cops, Pay Teachers, Buy Books...         3118       8  \n",
       "1  People Sent This Lesbian Questions And Her Rai...         4587     130  \n",
       "2  $3 Million Is What It Takes For A State To Leg...         3017      19  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/upworthy-archive-confirmatory-packages-03.12.2020.csv',\n",
    "                 usecols=['created_at','clickability_test_id','headline','impressions','clicks'])\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d69649c4-55f8-4ffc-8587-09552d84597e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39minfo(),df\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.info(),df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "973e21f4-3172-4257-80bd-2266ebe801ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "\n",
    "# read the CSV file and select columns of interest\n",
    "df = pd.read_csv('../data/upworthy-archive-exploratory-packages-03.12.2020.csv', usecols=['created_at','clickability_test_id', 'headline', 'impressions', 'clicks'])\n",
    "\n",
    "# create an empty DataFrame to store the results\n",
    "final_df = pd.DataFrame(columns=['clickability_test_id', 'headline A', 'headline B', 'impressions A', 'impressions B', 'clicks A', 'clicks B'])\n",
    "\n",
    "# group the DataFrame by clickability_test_id\n",
    "grouped = df.groupby('clickability_test_id')\n",
    "\n",
    "# iterate over each unique clickability_test_id\n",
    "for test_id, group in grouped:\n",
    "    # generate all possible pairs of headlines\n",
    "    pairs = list(combinations(group['created_at'], 2))\n",
    "    \n",
    "    # iterate over each pair and compute the impressions and clicks for each headline\n",
    "    for pair in pairs:\n",
    "        impressions_A = group.loc[group['created_at'] == pair[0], 'impressions'].iloc[0]\n",
    "        clicks_A = group.loc[group['created_at'] == pair[0], 'clicks'].iloc[0]\n",
    "        impressions_B = group.loc[group['created_at'] == pair[1], 'impressions'].iloc[0]\n",
    "        clicks_B = group.loc[group['created_at'] == pair[1], 'clicks'].iloc[0]\n",
    "        headline_A = group.loc[group['created_at'] == pair[0], 'headline'].iloc[0]\n",
    "        headline_B = group.loc[group['created_at'] == pair[1], 'headline'].iloc[0]\n",
    "        \n",
    "        # add the results to the final DataFrame\n",
    "        final_df = final_df.append({'clickability_test_id': test_id,\n",
    "                                    'headline A': headline_A,\n",
    "                                    'headline B': headline_B,\n",
    "                                    'impressions A': impressions_A,\n",
    "                                    'impressions B': impressions_B,\n",
    "                                    'clicks A': clicks_A,\n",
    "                                    'clicks B': clicks_B}, ignore_index=True)\n",
    "\n",
    "# display the final DataFrame\n",
    "# print(final_df.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b2c520e-874a-4d10-800e-f92416db87d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df[(final_df['clicks A'] != 0) | (final_df['clicks B'] != 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ef86d610-4929-4ab1-8319-f5fd9c111d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44798, 7)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c0d1d089-387e-46bb-8ddc-0b0c35664993",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df[final_df['headline A']!=final_df['headline B']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "05614fd3-a3a3-4837-8f3d-1d142f982a52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clickability_test_id</th>\n",
       "      <th>headline A</th>\n",
       "      <th>headline B</th>\n",
       "      <th>impressions A</th>\n",
       "      <th>impressions B</th>\n",
       "      <th>clicks A</th>\n",
       "      <th>clicks B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51436061220cb800020001e7</td>\n",
       "      <td>Creationism Has Nothing To Do With Christianit...</td>\n",
       "      <td>The One Where A Creationist Picks A Fight And ...</td>\n",
       "      <td>2551</td>\n",
       "      <td>2629</td>\n",
       "      <td>39</td>\n",
       "      <td>68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51436061220cb800020001e7</td>\n",
       "      <td>Creationism Has Nothing To Do With Christianit...</td>\n",
       "      <td>Creationism Shouldn't Be Taught In Science Cla...</td>\n",
       "      <td>2551</td>\n",
       "      <td>2539</td>\n",
       "      <td>39</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51436061220cb800020001e7</td>\n",
       "      <td>Creationism Has Nothing To Do With Christianit...</td>\n",
       "      <td>God Finds Out About Creationism And Sends A Re...</td>\n",
       "      <td>2551</td>\n",
       "      <td>2661</td>\n",
       "      <td>39</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>51436061220cb800020001e7</td>\n",
       "      <td>The One Where A Creationist Picks A Fight And ...</td>\n",
       "      <td>Creationism Shouldn't Be Taught In Science Cla...</td>\n",
       "      <td>2629</td>\n",
       "      <td>2539</td>\n",
       "      <td>68</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51436061220cb800020001e7</td>\n",
       "      <td>The One Where A Creationist Picks A Fight And ...</td>\n",
       "      <td>God Finds Out About Creationism And Sends A Re...</td>\n",
       "      <td>2629</td>\n",
       "      <td>2661</td>\n",
       "      <td>68</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       clickability_test_id  \\\n",
       "0  51436061220cb800020001e7   \n",
       "1  51436061220cb800020001e7   \n",
       "2  51436061220cb800020001e7   \n",
       "3  51436061220cb800020001e7   \n",
       "4  51436061220cb800020001e7   \n",
       "\n",
       "                                          headline A  \\\n",
       "0  Creationism Has Nothing To Do With Christianit...   \n",
       "1  Creationism Has Nothing To Do With Christianit...   \n",
       "2  Creationism Has Nothing To Do With Christianit...   \n",
       "3  The One Where A Creationist Picks A Fight And ...   \n",
       "4  The One Where A Creationist Picks A Fight And ...   \n",
       "\n",
       "                                          headline B impressions A  \\\n",
       "0  The One Where A Creationist Picks A Fight And ...          2551   \n",
       "1  Creationism Shouldn't Be Taught In Science Cla...          2551   \n",
       "2  God Finds Out About Creationism And Sends A Re...          2551   \n",
       "3  Creationism Shouldn't Be Taught In Science Cla...          2629   \n",
       "4  God Finds Out About Creationism And Sends A Re...          2629   \n",
       "\n",
       "  impressions B clicks A clicks B  \n",
       "0          2629       39       68  \n",
       "1          2539       39       49  \n",
       "2          2661       39       63  \n",
       "3          2539       68       49  \n",
       "4          2661       68       63  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3ea27a37-de13-4f12-823c-c20a8e83d151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21770, 7)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d51868e3-9409-47a4-94e6-2814ea5cf48b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2607"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['clickability_test_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9621e1d4-e8cb-417b-8eec-39b777551d97",
   "metadata": {},
   "source": [
    "Test statistics: Z - test\n",
    "$$ t = \\frac{(\\bar{\\pi_1} - \\bar{\\pi_2})}{\\sqrt{\\hat{\\pi}(1-\\hat{\\pi})(\\frac{1}{n_1}+\\frac{1}{n_2})}} \\sim N(0,1)$$\n",
    "where,\n",
    "$$ \\hat{\\pi} = \\frac{n_1\\hat{\\pi_1}+n_2\\hat{\\pi_2}}{n_1+n_2} $$\n",
    "\n",
    "We have normal distribution N(0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02b318d-cde9-4142-b99a-35a7bb5199fb",
   "metadata": {},
   "source": [
    "Condition to check: $\\pi_1>\\pi_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "75dfe7c9-3ecc-4edb-9872-3e30cb33e4ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clickability_test_id</th>\n",
       "      <th>headline A</th>\n",
       "      <th>headline B</th>\n",
       "      <th>impressions A</th>\n",
       "      <th>impressions B</th>\n",
       "      <th>clicks A</th>\n",
       "      <th>clicks B</th>\n",
       "      <th>p_a_gte_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51436061220cb800020001e7</td>\n",
       "      <td>Creationism Has Nothing To Do With Christianit...</td>\n",
       "      <td>The One Where A Creationist Picks A Fight And ...</td>\n",
       "      <td>2551</td>\n",
       "      <td>2629</td>\n",
       "      <td>39</td>\n",
       "      <td>68</td>\n",
       "      <td>0.003727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51436061220cb800020001e7</td>\n",
       "      <td>Creationism Has Nothing To Do With Christianit...</td>\n",
       "      <td>Creationism Shouldn't Be Taught In Science Cla...</td>\n",
       "      <td>2551</td>\n",
       "      <td>2539</td>\n",
       "      <td>39</td>\n",
       "      <td>49</td>\n",
       "      <td>0.136178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51436061220cb800020001e7</td>\n",
       "      <td>Creationism Has Nothing To Do With Christianit...</td>\n",
       "      <td>God Finds Out About Creationism And Sends A Re...</td>\n",
       "      <td>2551</td>\n",
       "      <td>2661</td>\n",
       "      <td>39</td>\n",
       "      <td>63</td>\n",
       "      <td>0.014438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>51436061220cb800020001e7</td>\n",
       "      <td>The One Where A Creationist Picks A Fight And ...</td>\n",
       "      <td>Creationism Shouldn't Be Taught In Science Cla...</td>\n",
       "      <td>2629</td>\n",
       "      <td>2539</td>\n",
       "      <td>68</td>\n",
       "      <td>49</td>\n",
       "      <td>0.943685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51436061220cb800020001e7</td>\n",
       "      <td>The One Where A Creationist Picks A Fight And ...</td>\n",
       "      <td>God Finds Out About Creationism And Sends A Re...</td>\n",
       "      <td>2629</td>\n",
       "      <td>2661</td>\n",
       "      <td>68</td>\n",
       "      <td>63</td>\n",
       "      <td>0.695843</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       clickability_test_id  \\\n",
       "0  51436061220cb800020001e7   \n",
       "1  51436061220cb800020001e7   \n",
       "2  51436061220cb800020001e7   \n",
       "3  51436061220cb800020001e7   \n",
       "4  51436061220cb800020001e7   \n",
       "\n",
       "                                          headline A  \\\n",
       "0  Creationism Has Nothing To Do With Christianit...   \n",
       "1  Creationism Has Nothing To Do With Christianit...   \n",
       "2  Creationism Has Nothing To Do With Christianit...   \n",
       "3  The One Where A Creationist Picks A Fight And ...   \n",
       "4  The One Where A Creationist Picks A Fight And ...   \n",
       "\n",
       "                                          headline B impressions A  \\\n",
       "0  The One Where A Creationist Picks A Fight And ...          2551   \n",
       "1  Creationism Shouldn't Be Taught In Science Cla...          2551   \n",
       "2  God Finds Out About Creationism And Sends A Re...          2551   \n",
       "3  Creationism Shouldn't Be Taught In Science Cla...          2629   \n",
       "4  God Finds Out About Creationism And Sends A Re...          2629   \n",
       "\n",
       "  impressions B clicks A clicks B  p_a_gte_b  \n",
       "0          2629       39       68   0.003727  \n",
       "1          2539       39       49   0.136178  \n",
       "2          2661       39       63   0.014438  \n",
       "3          2539       68       49   0.943685  \n",
       "4          2661       68       63   0.695843  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n1 = final_df['impressions A']\n",
    "n2 = final_df['impressions B']\n",
    "pi1hat = final_df['clicks A']/n1\n",
    "pi2hat = final_df['clicks B']/n2\n",
    "pihat = (n1*pi1hat + n2*pi2hat) / (n1 + n2)\n",
    "denom = pihat * (1-pihat) * (1/n1 + 1/n2)\n",
    "denom = denom.astype(float)\n",
    "t = (pi1hat - pi2hat) / np.sqrt(denom)\n",
    "t=t.astype(float)\n",
    "pv = stats.norm.cdf(t)\n",
    "final_df['p_a_gte_b'] = pv\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01ff0e3f-961b-4351-8b2d-f4f6a26fff80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bow_vectorizer(df):\n",
    "    corpus = np.concatenate([df['headline A'].values,\n",
    "                             df['headline B'].values])\n",
    "    vectorizer = CountVectorizer(ngram_range=(1,2),\n",
    "                                 lowercase=True,\n",
    "                                 binary=True,\n",
    "                                 max_df=0.6,\n",
    "                                 min_df=0.005)\n",
    "    vectorizer.fit(corpus)\n",
    "    return vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd61c051-22e6-4366-a5e3-e9b601abeada",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vector = build_bow_vectorizer(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9736f3d-5af7-4cde-a08e-a5d811504e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = bow_vector.get_feature_names()\n",
    "corpuss = np.concatenate([final_df['headline A'].values,final_df['headline B'].values],axis=0)\n",
    "bow = bow_vector.transform(corpuss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f96e52e-e45f-41c1-a5cf-dff4665319ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43540, 519)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d950cf9-b93c-44bb-9540-2dcad6a6eab1",
   "metadata": {},
   "source": [
    "We have 519 features and we have '1' if the feature is avaiable in headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8923a164-0e5c-4380-8c0f-ab7ac37878ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21770, 8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9861262f-c727-4765-8264-a7ff03dfd7de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The One Where A Creationist Picks A Fight And Loses To A Priest'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpuss[21770]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e2d8bcc-9074-4a6c-92e0-ea65d22be46d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The One Where A Creationist Picks A Fight And Loses To A Priest',\n",
       " 'God Finds Out About Creationism And Sends A Representative To Put A Stop To It')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpuss[4],corpuss[final_df.shape[0]+4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b07100-2519-47fc-9ab9-85737ff489b3",
   "metadata": {},
   "source": [
    "Modelling difference between 2 headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1443ace6-6e04-4425-b50a-b06e073f8ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bow[0].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0fc468c6-99be-4752-bef9-dc1501354737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# headline_embedd=np.empty(final_df.shape[0],dtype='int')\n",
    "headline_embedd=[]\n",
    "for i in range(final_df.shape[0]):\n",
    "    headline_embedd.append((bow[i]-bow[i+final_df.shape[0]]).toarray()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "040a665b-6204-488b-b87f-db576d26ecc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (bow[0]-bow[final_df.shape[0]]).toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd7efb99-9c0f-4bbc-82b1-520744c8af7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21770, 519)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headline_embedd = np.array(headline_embedd)\n",
    "headline_embedd.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4797c77-0cb1-4eaf-8be8-fd399a0599b3",
   "metadata": {},
   "source": [
    "We have only 21k records which are having different headlines for exploratory csv.<br>\n",
    "104k records in confirmatory csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b95a7ee-d4a7-4ce2-aeca-bf67ba195c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(p):\n",
    "    return np.log(p / (1 - p))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a47c03d-f2e0-4d23-a02c-48defad0830f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df[final_df['p_a_gte_b']==epsilon]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d067f43c-9311-4306-9bec-13685e04587d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-8\n",
    "# Update values in 'p_a_gte_b' column of final_df where condition is met\n",
    "final_df.loc[final_df['p_a_gte_b'] == 1, 'p_a_gte_b'] = epsilon  #So that logit doesnt become inf\n",
    "\n",
    "X=headline_embedd\n",
    "Y=final_df['p_a_gte_b']\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,Y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c01f146b-a890-4ea6-84f0-ec5797b378b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train = logit(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c08a06b7-ff57-4694-80e0-cc535e0ea5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = LinearRegression().fit(x_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "55be7f1b-996d-4f2e-9e34-9a0b4164a1f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16234364109562108"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = base_model.predict(x_test)\n",
    "y_pred_pv = sigmoid(y_pred)\n",
    "mse_bow = mean_squared_error(y_test,y_pred_pv)\n",
    "mse_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63929a7-cc56-4e15-9f73-1f11471bdf4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad91e183-9ab4-4707-9b72-19a07fe3017b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6302a480-7f98-4d86-8820-b7f4853fe627",
   "metadata": {},
   "source": [
    "0.16234 - MSE from bow vectorization = exploratory data<br>\n",
    "0.1562 - MSE from bow vectorization = confirmatory data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda162a5-7577-48d9-8ade-6b9332268fe0",
   "metadata": {},
   "source": [
    "## Using sentence transformer instead of Distilbert last hidden state activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4a125fc6-c471-40c9-8825-c34ee457df82",
   "metadata": {},
   "outputs": [],
   "source": [
    "## does the sentence transformer get the embedding with what words are in what position as it takes whole sentence. \n",
    "# hows it diffferent from tokenizer and model fit by distilbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8238ad-a157-46bd-81ca-442900f0acc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4a05ce29-83f3-4023-97fe-bbb45e6d45bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "009b23307b0e44a5a81ab8c6b82f67ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3278 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38c4da87fa664bf792b9627d8e299595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3278 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "text_A = list(final_df['headline A'].values)\n",
    "text_B = list(final_df['headline B'].values)\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "embeddings_A = model.encode(text_A,output_value='sentence_embedding',show_progress_bar=True)\n",
    "embeddings_B = model.encode(text_B,output_value='sentence_embedding',show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6c024256-4172-4a29-b3e6-95f47130fbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_headline_embedd = embeddings_A-embeddings_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "218a07b7-af71-4ee8-88fc-cb31a44b05a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=sent_headline_embedd\n",
    "Y=final_df['p_a_gte_b']\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,Y,test_size=0.2,random_state=42)\n",
    "target_train = logit(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cd424503-b6c8-45fb-8e7c-0fffb1605a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = LinearRegression().fit(x_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "487ee6c9-723a-44a8-aa9f-ee2efa7086d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1507556584095765"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = base_model.predict(x_test)\n",
    "y_pred_pv = sigmoid(y_pred)\n",
    "mse_sentence_transformer = mean_squared_error(y_test,y_pred_pv)\n",
    "mse_sentence_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0226d5e5-f84d-4678-b996-a2b53a24323b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.56126400542451"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(logit(y_test),y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3249fd8-5b78-443b-896b-f6264c73e8c4",
   "metadata": {},
   "source": [
    "0.159 MSE with sentence transformer embeddings - exploratory<br>\n",
    "0.150 MSE with sentence transformer embeddings - confirmatory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0743d7b3-70dd-4020-94da-c7f75b2e2f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmsAAAGdCAYAAABNbzR9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABQYklEQVR4nO3dd1xT9/4/8NdJIERGokxREHFvBERZqdqBirNa96x1tSpSb3tbLx3a3lt721ur4Gi1A23rbB2oVEtbB4g4kLgHKggqCKgQQAkj5/eHv5vvpbhA9ATyej4e5/EwJ59z8kro45FXz5sEQRRFEURERERkkmRSByAiIiKiB2NZIyIiIjJhLGtEREREJoxljYiIiMiEsawRERERmTCWNSIiIiITxrJGREREZMJY1oiIiIhMmIXUAZ4lg8GA69evw87ODoIgSB2HiIiIHoMoiigsLESTJk0gk5nfdSazKmvXr1+Hu7u71DGIiIioBjIzM+Hm5iZ1jGfOrMqanZ0dgHs/bJVKJXEaIiIiehw6nQ7u7u7G93FzY1Zl7b+jT5VKxbJGRERUx5jrrzCZ3+CXiIiIqA5hWSMiIiIyYSxrRERERCaMZY2IiIjIhLGsEREREZkwljUiIiIiE8ayRkRERGTCWNaIiIiITBjLGhEREZEJY1kjIiIiMmEsa0REREQmjGWNiIiIyISxrNWC7w+k4aPtZ1BabpA6ChEREdUzFlIHqOuyC0qwMPYcSisMSL5yC0vH+MDd3lrqWERERFRP8MraE2qsVmLZWB+oG1ji+NUChEbGY9epLKljERERUT3BslYLXurggp1hwfBp1hCFJeWY8eMxfLjtFErKKqSORkRERHUcy1otcWtkjQ3TAzC9ZwsAwOqDVzBsRSLS84olTkZERER1GctaLbKUyzCvX3t8P8kPjawtcfq6DgOiEhBz/LrU0YiIiKiOYll7Cnq3c0bsHA38mjdCkb4cYetSMG/zSY5FiYiIqNpY1p4SV3UDrJvqj1m9W0EQgHWHMzBk2QFcyi2SOhoRERHVISxrT5GFXIa3+rTFmsnd4WCjwLnsQgyMSsCWlKtSRyMiIqI6gmXtGdC0dsKvczQIaOGAO6UVeHPDcby96TjulnIsSkRERA9Xo7K2fPlyeHp6QqlUwtfXF/Hx8Q9cm5WVhTFjxqBt27aQyWQIDw+vsiY6OhqCIFTZSkpKKq27du0axo0bBwcHB1hbW6Nr165ITk6uyVN45pxVSvw4pQfCX2wNQQA2JV/FoKUJuHCjUOpoREREZMKqXdY2bNiA8PBwREREICUlBRqNBv369UNGRsZ91+v1ejg5OSEiIgJeXl4PPK9KpUJWVlalTalUGu+/ffs2goKCYGlpiV9//RVnzpzBF198gYYNG1b3KUhGLhMQ/mIb/DSlB5zsrJCaU4RBSxOw8WgmRFGUOh4RERGZIEGsZkvo0aMHfHx8sGLFCuO+9u3bY8iQIVi4cOFDj+3Vqxe6du2KxYsXV9ofHR2N8PBw5OfnP/DYd999FwcOHHjoVbxH0el0UKvVKCgogEqlqvF5akNuoR5zN2oRn5oHABjq3RQfD+kEGyv+BTAiIqL/ZUrv31Ko1pW10tJSJCcnIyQkpNL+kJAQJCYmPlGQoqIieHh4wM3NDQMGDEBKSkql+2NiYtCtWzcMHz4czs7O8Pb2xqpVqx56Tr1eD51OV2kzFU52Vlj9ane83actZAKwOeUaBi5NwNks08lIRERE0qtWWcvLy0NFRQVcXFwq7XdxcUF2dnaNQ7Rr1w7R0dGIiYnBunXroFQqERQUhNTUVOOay5cvY8WKFWjdujV2796NGTNmICwsDGvWrHngeRcuXAi1Wm3c3N3da5zxaZDJBMzs3QrrpwWgsUqJy7nFGLLsANYeyuBYlIiIiADU8AMGgiBUui2KYpV91eHv749x48bBy8sLGo0GGzduRJs2bRAVFWVcYzAY4OPjg08++QTe3t6YPn06pk6dWmkc+1fz5s1DQUGBccvMzKxxxqepu6c9Yudo0KutE/TlBvxjy0mErdeisKRM6mhEREQksWqVNUdHR8jl8ipX0XJycqpcbXuiUDIZ/Pz8Kl1Zc3V1RYcOHSqta9++/QM/2AAAVlZWUKlUlTZTZW+jwHcT/TCvXzvIZQK2H7+OgVEJOHWtQOpoREREJKFqlTWFQgFfX1/ExcVV2h8XF4fAwMBaCyWKIrRaLVxdXY37goKCcP78+UrrLly4AA8Pj1p7XKnJZAKm92yJjdMD0EStRPrNOxi6PBFrDqZzLEpERGSmqj0GnTt3Lr755ht89913OHv2LN58801kZGRgxowZAO6NHidMmFDpGK1WC61Wi6KiIuTm5kKr1eLMmTPG+xcsWIDdu3fj8uXL0Gq1eO2116DVao3nBIA333wTSUlJ+OSTT3Dx4kWsXbsWK1euxMyZM2v63E2Wr0cjxM7R4MX2LiitMOCDbafxxk/HUHCXY1EiIiJzU+2v7gDufSnuZ599hqysLHTq1AlffvklnnvuOQDApEmTkJ6ejr179/7fg9zn99k8PDyQnp4O4F4R27x5M7Kzs6FWq+Ht7Y358+cjICCg0jE7duzAvHnzkJqaCk9PT8ydOxdTp0597Nx17aO/oiji24Q0/HvXOZRViHC3b4Clo33g5d5Q6mhERETPTF17/65tNSprdVVd/WFrM/Mxa+0xXL19F5ZyAe/2a4/JQc2f6EMdREREdUVdff+uLfzboHVAV/eG2BmmQd+OjVFWIeLjHWcwdU0y8u+USh2NiIiInjKWtTpC3cASK8b54KPBHaGQy/D72RvoH5mA5Cu3pY5GRERETxHLWh0iCAImBDTH5jcC4eFgjWv5dzHy64P4et8lGAxmM80mIiIyKyxrdVCnpmrsmB2MAV1cUW4QsfDXc3ht9RHcKuZYlIiIqL5hWauj7JSWiBrtjU9e7gyFhQx7zucidEk8DqfdkjoaERER1SKWtTpMEASM6dEM22YGoYWjDbJ1JRi9KgnL9lzkWJSIiKieYFmrB9q7qrB9djBe9m6KCoOIz3efx8TvDyOvSC91NCIiInpCLGv1hI2VBRaN8MJnw7pAaSlDfGoe+i2JR+KlPKmjERER0RNgWatHBEHACD93xMwKRmtnW+QW6jHum0NY/PsFVHAsSkREVCexrNVDbVzssG1WEIb7usEgAot/T8X4bw8hR1cidTQiIiKqJpa1espaYYHPh3th0QgvWCvkSLx0E6GR8YhPzZU6GhEREVUDy1o9N9THDTGzgtGusR3yikox4bvD+M/u8yivMEgdjYiIiB4Dy5oZaOVsi60zgzC6ezOIIrB0z0WMWXUI2QUcixIREZk6ljUzobSUY+HQzogc7Q0bhRyH028hNDIee87nSB2NiIiIHoJlzcwM8mqCHWEadGyiwq3iUrz6/REs/PUsyjgWJSIiMkksa2bI09EGv7weiAkBHgCAr/ddxqiVSbiWf1fiZERERPRXLGtmSmkpx0eDO2H5WB/YWVkg+cpthC6JR9yZG1JHIyIiov/BsmbmQju7YmeYBl3c1Ci4W4apa47i4x1nUFrOsSgREZEpYFkjNHOwxs8zAjE5yBMA8G1CGoZ/fRCZt+5InIyIiIhY1ggAoLCQ4YOBHbByvC9USgscz8xHaGQ8dp3KkjoaERGRWWNZo0pCOjZG7BwNvJs1RGFJOWb8eAwfbjsFfXmF1NGIiIjMEssaVeHWyBobpwdg+nMtAACrD17BsBWJSM8rljgZERGR+WFZo/uylMswL7Q9vpvUDY2sLXHqmg4DohKw48R1qaMRERGZFZY1eqjn27kgdo4Gfs0boUhfjllrU/CPLSdRUsaxKBER0bPAskaP5KpugHVT/TGzd0sIArD2UAaGLDuAS7lFUkcjIiKq91jW6LFYyGV4u087rH61OxxsFDiXXYiBUQnYknJV6mhERET1GssaVctzbZwQO0cD/xb2uFNagTc3HMfffz6Ou6UcixIRET0NLGtUbS4qJX6a4o85L7SGIAAbj17F4GUJSL1RKHU0IiKieodljWpELhPw5ktt8NNrPeBkZ4ULN4owcGkCNh3NlDoaERFRvcKyRk8ksJUjYsM0CG7liJIyA97++QTmbtSiWF8udTQiIqJ6gWWNnpiTnRXWTO6Ot0LaQCYAm49dw6ClCTiXrZM6GhERUZ3Hska1QiYTMOv51lg31R8uKitcyi3G4KUHsO5wBkRRlDoeERFRncWyRrWqRwsHxIZp0LONE/TlBszbfBJh67UoLCmTOhoREVGdxLJGtc7B1grfT/LDu/3aQS4TsP34dQyMSsCpawVSRyMiIqpzWNboqZDJBMzo2RIbp/ujiVqJ9Jt3MHR5In44mM6xKBERUTWwrNFT5ethj51hGrzY3hmlFQa8v+00Zq49Bh3HokRERI+FZY2eukY2Cqya0A3v9W8PS7mA2JPZ6B8Zj+OZ+VJHIyIiMnksa/RMCIKAKZoW2DQjEG6NGiDz1l288lUivktI41iUiIjoIVjW6Jnq6t4QO8M06NuxMcoqRHy04wym/ZCM/DulUkcjIiIySSxr9MypG1hixTgfLBjUEQq5DHFnbqB/ZAKOZdyWOhoREZHJYVkjSQiCgImBzfHL64HwcLDGtfy7GPHVQazcfwkGA8eiRERE/8WyRpLq7KbGjtnB6N/FFeUGEZ/EnsOUNUdxq5hjUSIiIoBljUyAndISS0d7418vd4LCQoY/z+Wgf2Q8jqTfkjoaERGR5FjWyCQIgoCxPTyw9Y0gtHC0QVZBCUatTMKyPRc5FiUiIrPGskYmpUMTFWJmB2NI1yaoMIj4fPd5TPz+MPKK9FJHIyIikgTLGpkcWysLfDmyKz4b1gVKSxniU/MQuiQeBy/dlDoaERHRM8eyRiZJEASM8HNHzKxgtHK2RU6hHmO/ScKS31NRwbEoERGZEZY1MmltXOwQMysIw33dYBCBL3+/gPHfHkJOYYnU0YiIiJ4JljUyedYKC3w+3AuLRnihgaUciZduInRJPBJS86SORkRE9NSxrFGdMdTHDdtnB6NdYzvkFZVi/HeH8MVv51FeYZA6GhER0VNTo7K2fPlyeHp6QqlUwtfXF/Hx8Q9cm5WVhTFjxqBt27aQyWQIDw+vsiY6OhqCIFTZSkr+b9Q1f/78Kvc3bty4JvGpDmvlbIutM4Mwurs7RBGI+vMixnxzCNkFHIsSEVH9VO2ytmHDBoSHhyMiIgIpKSnQaDTo168fMjIy7rter9fDyckJERER8PLyeuB5VSoVsrKyKm1KpbLSmo4dO1a6/+TJk9WNT/WA0lKOhUO7YMmorrBRyHE47RZCI+Ox93yO1NGIiIhqXbXL2qJFi/Daa69hypQpaN++PRYvXgx3d3esWLHivuubN2+OJUuWYMKECVCr1Q8873+vlP3v9lcWFhaV7ndycqpufKpHBndtih1hGnRwVeFWcSkmfX8En/56DmUcixIRUT1SrbJWWlqK5ORkhISEVNofEhKCxMTEJwpSVFQEDw8PuLm5YcCAAUhJSamyJjU1FU2aNIGnpydGjRqFy5cvP/Scer0eOp2u0kb1i6ejDTa/EYjx/h4AgK/2XcKolUm4nn9X4mRERES1o1plLS8vDxUVFXBxcam038XFBdnZ2TUO0a5dO0RHRyMmJgbr1q2DUqlEUFAQUlNTjWt69OiBNWvWYPfu3Vi1ahWys7MRGBiImzcf/EWpCxcuhFqtNm7u7u41zkimS2kpx8dDOmH5WB/YWVkg+cpthEbG4/czN6SORkRE9MRq9AEDQRAq3RZFscq+6vD398e4cePg5eUFjUaDjRs3ok2bNoiKijKu6devH4YNG4bOnTvjxRdfxM6dOwEAq1evfuB5582bh4KCAuOWmZlZ44xk+kI7u2JnmAZd3NTIv1OGKWuO4p87zqC0nGNRIiKqu6pV1hwdHSGXy6tcRcvJyalyte2JQslk8PPzq3Rl7a9sbGzQuXPnh66xsrKCSqWqtFH91szBGptmBGBykCcA4JuENAz/+iAyb92ROBkREVHNVKusKRQK+Pr6Ii4urtL+uLg4BAYG1looURSh1Wrh6ur6wDV6vR5nz5596BoyT1YWcnwwsANWjveFSmmB45n56B8Zj12naj6qJyIikkq1x6Bz587FN998g++++w5nz57Fm2++iYyMDMyYMQPAvdHjhAkTKh2j1Wqh1WpRVFSE3NxcaLVanDlzxnj/ggULsHv3bly+fBlarRavvfYatFqt8ZwA8NZbb2Hfvn1IS0vDoUOH8Morr0Cn02HixIk1fe5Uz4V0bIzYORp4N2sIXUk5ZvyYjPkxp6Evr5A6GhER0WOzqO4BI0eOxM2bN/HRRx8hKysLnTp1QmxsLDw87n0aLysrq8p3rnl7exv/nZycjLVr18LDwwPp6ekAgPz8fEybNg3Z2dlQq9Xw9vbG/v370b17d+NxV69exejRo5GXlwcnJyf4+/sjKSnJ+LhE9+PWyBobpwfg893nsXL/ZUQnpiP5ym0sHeMNDwcbqeMRERE9kiCKoih1iGdFp9NBrVajoKCAv79mhv48dwN/23gct++UwdbKAp8O64wBXZpIHYuIiB7B3N+/+bdByWw8384FsXM06ObRCEX6csxam4KILSdRUsaxKBERmS6WNTIrruoGWD/NH2/0agkA+OlQBl5enojLuUUSJyMiIro/ljUyOxZyGf7etx1WT+4OBxsFzmbpMCAqAVtTrkkdjYiIqAqWNTJbPds4IXaOBv4t7HGntALhG7R45+cTuFvKsSgREZkOljUyay4qJX6a4o+wF1pDEIANRzMxZNkBXMwplDoaERERAJY1IshlAua+1AY/vdYDjrZWOH+jEAOjDuDn5KtSRyMiImJZI/qvwFaO+HWOBsGtHHG3rAJvbTqOuRu1KNaXSx2NiIjMGMsa0f9wsrPC6snd8beX2kAmAJuPXcOgpQk4l62TOhoREZkpljWiv5DLBMx+oTXWTvWHi8oKl3KLMXjpAaw/nAEz+g5pIiIyESxrRA/g38IBsWEa9GzjBH25Ae9uPok567Uo4liUiIieIZY1oodwsLXC95P88E7fdpDLBMQcv46BUQk4fb1A6mhERGQmWNaIHkEmE/B6r5bYON0fTdRKpOUV4+Xlifgh6QrHokRE9NSxrBE9Jl8Pe+wM0+DF9s4oLTfg/a2nMGttCnQlZVJHIyKieoxljagaGtkosGpCN7zXvz0sZAJ2nszCgMgEnLiaL3U0IiKqp1jWiKpJEARM0bTAphkBaNqwATJu3cGwFYn4/kAax6JERFTrWNaIasi7WSPEhmnQp6MLyipELNh+BtN/SEbBHY5FiYio9rCsET0BtbUlvhrni/kDO0Ahl+G3MzcQGhmPlIzbUkcjIqJ6gmWN6AkJgoBJQZ745fVANLO3xrX8uxj+1UGs2n8ZBgPHokRE9GRY1ohqSWc3NXaEBaN/F1eUG0T8K/Yspqw5itvFpVJHIyKiOoxljagWqZSWWDraG/8c0gkKCxn+PJeD0Mh4HE2/JXU0IiKqo1jWiGqZIAgY5++BrW8EoYWjDbIKSjByZRKW773IsSgREVUbyxrRU9KhiQoxs4MxpGsTVBhEfLbrPCZFH0FekV7qaEREVIewrBE9RbZWFvhyZFf8e1hnKC1l2H8hF6FL4pF0+abU0YiIqI5gWSN6ygRBwEi/Ztg2MxitnG2RU6jHmFVJiPwjFRUcixIR0SOwrBE9I20b2yFmVhBe8XWDQQQWxV3AhO8OIaewROpoRERkwljWiJ4ha4UF/jPcC18M90IDSzkOXLyJ0CUJOHAxT+poRERkoljWiCQwzNcN22cHoa2LHfKK9Bj37SEs+u08x6JERFQFyxqRRFo522HbrCCM7u4OUQQi/7yIMauScEPHsSgREf0fljUiCSkt5Vg4tAuWjOoKG4Uch9Juod+SeOw9nyN1NCIiMhEsa0QmYHDXptg+OxjtXVW4VVyKSd8fwb93nUN5hUHqaEREJDGWNSIT0cLJFlveCMR4fw8AwIq9lzBqZRKu59+VOBkREUmJZY3IhCgt5fh4SCcsG+MDOysLHL1yG6GR8fjj7A2poxERkURY1ohMUP8urtgRFozOTdXIv1OG11Yfxb92nkFpOceiRETmhmWNyER5ONjg59cD8GpQcwDAqvg0jPj6IDJv3ZE2GBERPVMsa0QmzMpCjg8HdsTX432hUlpAm5mP/pHx2H06W+poRET0jLCsEdUBfTo2xs4wDbq6N4SupBzTf0jG/JjT0JdXSB2NiIieMpY1ojrC3d4aG6cHYKrGEwAQnZiOV1YcxJWbxRInIyKip4lljagOUVjIENG/A76d2A0NrS1x8loBBkQmYOeJLKmjERHRU8KyRlQHvdDeBbFhGnTzaIRCfTlmrj2G97aeREkZx6JERPUNyxpRHdWkYQOsm+aPN3q1BAD8mJSBl5cn4nJukcTJiIioNrGsEdVhlnIZ/t63HVZP7g4HGwXOZukwMCoB27TXpI5GRES1hGWNqB7o2cYJsXM06OFpj+LSCsxZr8W7v5zA3VKORYmI6jqWNaJ6wkWlxE9TeiDshdYQBGD9kUwMWXYAF3MKpY5GRERPgGWNqB6xkMsw96U2+PG1HnC0tcL5G4UYGHUAPydflToaERHVEMsaUT0U1MoRsXOCEdTKAXfLKvDWpuP428bjuFNaLnU0IiKqJpY1onrK2U6JNZN7YO5LbSATgF+OXcWgpQdwPptjUSKiuoRljagek8sEhL3QGmun+sNFZYWLOUUYtDQBG45kQBRFqeMREdFjYFkjMgP+LRwQG6bBc22coC834J1fTuLNDVoU6TkWJSIydSxrRGbCwdYK0ZP88Pe+bSGXCdiqvY5BUQk4c10ndTQiInoIljUiMyKTCXijVytsmOYPV7USl/OKMWT5AfyYdIVjUSIiE8WyRmSGujW3R2yYBi+0c0ZpuQHvbT2FWetSoCspkzoaERH9RY3K2vLly+Hp6QmlUglfX1/Ex8c/cG1WVhbGjBmDtm3bQiaTITw8vMqa6OhoCIJQZSspKbnvORcuXAhBEO57LiJ6PI1sFPhmYje81789LGQCdp7IwoDIBJy8WiB1NCIi+h/VLmsbNmxAeHg4IiIikJKSAo1Gg379+iEjI+O+6/V6PZycnBAREQEvL68HnlelUiErK6vSplQqq6w7cuQIVq5ciS5dulQ3OhH9hSAImKJpgU0zAtC0YQNk3LqDYSsSEX0gjWNRIiITUe2ytmjRIrz22muYMmUK2rdvj8WLF8Pd3R0rVqy47/rmzZtjyZIlmDBhAtRq9QPPKwgCGjduXGn7q6KiIowdOxarVq1Co0aNqhudiB7Au1kjxIZpENLBBaUVBszffgYzfkxGwR2ORYmIpFatslZaWork5GSEhIRU2h8SEoLExMQnClJUVAQPDw+4ublhwIABSElJqbJm5syZ6N+/P1588cXHOqder4dOp6u0EdH9qa0t8fV4X3w4sAMs5QJ2n76B/lHxSMm4LXU0IiKzVq2ylpeXh4qKCri4uFTa7+Liguzs7BqHaNeuHaKjoxETE4N169ZBqVQiKCgIqampxjXr16/HsWPHsHDhwsc+78KFC6FWq42bu7t7jTMSmQNBEPBqkCd+eT0QzeytcfX2XQz/6iC+ib/MsSgRkURq9AEDQRAq3RZFscq+6vD398e4cePg5eUFjUaDjRs3ok2bNoiKigIAZGZmYs6cOfjxxx/v+3tsDzJv3jwUFBQYt8zMzBpnJDInXdwaYkdYMPp3dkW5QcQ/d57FlNVHcbu4VOpoRERmp1plzdHREXK5vMpVtJycnCpX254olEwGPz8/45W15ORk5OTkwNfXFxYWFrCwsMC+ffsQGRkJCwsLVFRU3Pc8VlZWUKlUlTYiejwqpSWWjvHGx0M6QWEhwx/nctA/Mh7JV25JHY2IyKxUq6wpFAr4+voiLi6u0v64uDgEBgbWWihRFKHVauHq6goAeOGFF3Dy5ElotVrj1q1bN4wdOxZarRZyubzWHpuI/o8gCBjv74EtbwTC09EG1wtKMOLrJKzYewkGA8eiRETPgkV1D5g7dy7Gjx+Pbt26ISAgACtXrkRGRgZmzJgB4N7o8dq1a1izZo3xGK1WC+Dehwhyc3Oh1WqhUCjQoUMHAMCCBQvg7++P1q1bQ6fTITIyElqtFsuWLQMA2NnZoVOnTpVy2NjYwMHBocp+Iqp9HZuosX12MCK2nMQ27XX8e9c5JF2+iUUjvOBgayV1PCKieq3aZW3kyJG4efMmPvroI2RlZaFTp06IjY2Fh4cHgHtfgvvX71zz9vY2/js5ORlr166Fh4cH0tPTAQD5+fmYNm0asrOzoVar4e3tjf3796N79+5P8NSIqDbZWllg8ciuCGzpgA+2nca+C7kIjYxH5Chv9GjhIHU8IqJ6SxDN6CNeOp0OarUaBQUF/P01oidwPrsQb/yUjEu5xZAJwJsvtsEbvVtBLqv5B42IiB7E3N+/+bdBiaja2ja2w/bZwRjm4waDCHwRdwETvjuE3EK91NGIiOodljUiqhFrhQW+GOGF/wz3QgNLOQ5cvIl+S+Jx4GKe1NGIiOoVljUieiKv+LohZlYQ2rrYIa9Ij3HfHsKiuAuo4KdFiYhqBcsaET2x1i522DozCKP83CGKQOQfqRj7TRJu6EqkjkZEVOexrBFRrWigkOPTYV2wZFRX2CjkSLp8C6FL4rHvQq7U0YiI6jSWNSKqVYO7NsX22cFo76rCzeJSTPzuMD7bdQ7lFQapoxER1Uksa0RU61o42WLLG4EY598MALB87yWMXpWErIK7EicjIqp7WNaI6KlQWsrxzyGdsXSMN+ysLHAk/TZCl8Tjz3M3pI5GRFSnsKwR0VM1oEsT7AgLRuematy+U4bJ0UfxSexZlHEsSkT0WFjWiOip83Cwwc+vB2BSYHMAwMr9lzH8q4O4evuOtMGIiOoAljUieiasLOSYP6gjvhrnC5XSAtrMfIQuicfu09lSRyMiMmksa0T0TPXt1Bg7wzTwcm8IXUk5pv+QjAXbT6O0nGNRIqL7YVkjomfO3d4am6YHYKrGEwDw/YF0vPJVIjJucixKRPRXLGtEJAmFhQwR/Tvgmwnd0NDaEieuFqB/ZDxiT2ZJHY2IyKSwrBGRpF7s4ILYMA26eTRCob4cb/x0DO9vPYWSsgqpoxERmQSWNSKSXJOGDbBumj9e79USAPBD0hUMXZ6ItLxiiZMREUmPZY2ITIKlXIZ3+rZD9Kt+sLdR4EyWDgMi47FNe03qaEREkmJZIyKT0qutM2LDNOjuaY/i0grMWa/FvM0nOBYlIrPFskZEJqexWom1U3og7PlWEARg3eFMDFl2ABdziqSORkT0zLGsEZFJspDLMDekLX6Y3AOOtlY4l12IgVEJ+CX5qtTRiIieKZY1IjJpwa0dETsnGIEtHXC3rAJ/23Qcb206jjul5VJHIyJ6JljWiMjkOdsp8cNrPTD3pTaQCcDPyVcxeOkBXLhRKHU0IqKnjmWNiOoEuUxA2Aut8dMUfzjbWSE1pwiDliZgw5EMiKIodTwioqeGZY2I6pSAlg6InaPBc22cUFJmwDu/nMSbG7Qo0nMsSkT1E8saEdU5jrZWiJ7kh7/3bQu5TMBW7XUMikrAmes6qaMREdU6ljUiqpNkMgFv9GqF9dP84apW4nJeMYYsP4CfDl3hWJSI6hWWNSKq0/ya2yM2TIPn2zmjtNyAiC2nMGtdCgpLyqSORkRUK1jWiKjOa2SjwDcTuiEitD0sZAJ2nsjCgKgEnLpWIHU0IqInxrJGRPWCTCZg6nMtsHFGAJo2bIArN+9g6PJErE5M51iUiOo0ljUiqld8mjVCbJgGIR1cUFphwIcxp/H6j8dQcJdjUSKqm1jWiKjeUVtb4uvxvvhwYAdYygXsOp2N/pHx0GbmSx2NiKjaWNaIqF4SBAGvBnnil9cD0czeGldv38UrKxLxTfxljkWJqE5hWSOieq2LW0PsCAtGaOfGKDeI+OfOs5i65ijy75RKHY2I6LGwrBFRvadSWmLZGB98PKQTFBYy/H42B6FL4pF85ZbU0YiIHolljYjMgiAIGO/vgS1vBMLT0QbXC0ow4uskfLXvEgwGjkWJyHSxrBGRWenYRI3ts4MxyKsJKgwiPv31HCavPoKbRXqpoxER3RfLGhGZHVsrCywZ1RWfDu0MKwsZ9p7PRWhkPA5dvil1NCKiKljWiMgsCYKAUd2bYdusILR0ssENnR6jVyVh6Z+pHIsSkUlhWSMis9ausQoxs4Ix1KcpDCLwn98uYOL3h5FbyLEoEZkGljUiMns2VhZYNKIrPn+lCxpYyhGfmofQyHgkXsyTOhoREcsaEdF/De/mjphZQWjjYovcQj3GfnsIi+IuoIJjUSKSEMsaEdH/aO1ih20zgzHKzx2iCET+kYqx3yThhq5E6mhEZKZY1oiI/qKBQo5Ph3XBklFdYaOQI+nyLYQuicf+C7lSRyMiM8SyRkT0AIO7NsX22cFo76rCzeJSTPz+MD7ffQ7lFQapoxGRGWFZIyJ6iBZOttjyRiDG9mgGUQSW7bmE0auSkFVwV+poRGQmWNaIiB5BaSnHv17ujKVjvGFrZYEj6bcRuiQee87lSB2NiMwAyxoR0WMa0KUJdoYFo1NTFW7fKcOr0UewMPYsyjgWJaKniGWNiKgaPBxs8MvrgZgU2BwA8PX+yxjx9UFcvX1H2mBEVG+xrBERVZOVhRzzB3XEV+N8YKe0QEpGPvpHJuC309lSRyOieohljYiohvp2ckVsmAZe7g1RcLcM035Ixkfbz6C0nGNRIqo9LGtERE/A3d4am6YHYEqwJwDguwNpeOWrRGTc5FiUiGpHjcra8uXL4enpCaVSCV9fX8THxz9wbVZWFsaMGYO2bdtCJpMhPDy8ypro6GgIglBlKyn5v28MX7FiBbp06QKVSgWVSoWAgAD8+uuvNYlPRFSrFBYyvDegA76Z0A3qBpY4cbUA/SPj8evJLKmjEVE9UO2ytmHDBoSHhyMiIgIpKSnQaDTo168fMjIy7rter9fDyckJERER8PLyeuB5VSoVsrKyKm1KpdJ4v5ubGz799FMcPXoUR48exfPPP4/Bgwfj9OnT1X0KRERPxYsdXBA7RwNfj0Yo1Jfj9Z+O4YNtp1BSViF1NCKqwwRRFKv1F4p79OgBHx8frFixwrivffv2GDJkCBYuXPjQY3v16oWuXbti8eLFlfZHR0cjPDwc+fn51YkCe3t7fP7553jttdcea71Op4NarUZBQQFUKlW1HouI6HGVVRjwxW8X8NW+SwCAjk1UWDrGB56ONhInI6qbzP39u1pX1kpLS5GcnIyQkJBK+0NCQpCYmPhEQYqKiuDh4QE3NzcMGDAAKSkpD1xbUVGB9evXo7i4GAEBAQ9cp9frodPpKm1ERE+bpVyGd/u1Q/SrfrC3UeD0dR0GRiUg5vh1qaMRUR1UrbKWl5eHiooKuLi4VNrv4uKC7Oyaf2S9Xbt2iI6ORkxMDNatWwelUomgoCCkpqZWWnfy5EnY2trCysoKM2bMwJYtW9ChQ4cHnnfhwoVQq9XGzd3dvcYZiYiqq1dbZ8SGadDd0x5F+nKErUvBvM0nORYlomqp0QcMBEGodFsUxSr7qsPf3x/jxo2Dl5cXNBoNNm7ciDZt2iAqKqrSurZt20Kr1SIpKQmvv/46Jk6ciDNnzjzwvPPmzUNBQYFxy8zMrHFGIqKaaKxWYu2UHpj9fCsIArDucAaGLDuAizlFUkcjojqiWmXN0dERcrm8ylW0nJycKlfbniiUTAY/P78qV9YUCgVatWqFbt26YeHChfDy8sKSJUseeB4rKyvjp0f/uxERPWsWchn+FtIWP0zuAUdbBc5lF2LQ0gRsPnZV6mhEVAdUq6wpFAr4+voiLi6u0v64uDgEBgbWWihRFKHVauHq6vrIdXq9vtYel4joaQpu7YjYMA0CWzrgTmkF5m48jrc3Hced0nKpoxGRCbOo7gFz587F+PHj0a1bNwQEBGDlypXIyMjAjBkzANwbPV67dg1r1qwxHqPVagHc+xBBbm4utFotFAqF8ffNFixYAH9/f7Ru3Ro6nQ6RkZHQarVYtmyZ8Rz/+Mc/0K9fP7i7u6OwsBDr16/H3r17sWvXrid5/kREz5SzSokfXuuBpX9exJI/LmBT8lVoM/OxbKwP2rjYSR2PiExQtcvayJEjcfPmTXz00UfIyspCp06dEBsbCw8PDwD3vgT3r9+55u3tbfx3cnIy1q5dCw8PD6SnpwMA8vPzMW3aNGRnZ0OtVsPb2xv79+9H9+7djcfduHED48ePR1ZWFtRqNbp06YJdu3bhpZdeqsnzJiKSjFwmYM6LrdHd0x5z1qcgNacIg5Ym4KPBnTDc1+2JfgeYiOqfan/PWl1m7t/TQkSmJ69Ijzc3aBGfmgcAeNm7Kf45pBNsrKr9/9JE9Za5v3/zb4MSEUnI0dYKq1/tjrf7tIVcJmBLyjUMjErA2Sx+LyQR3cOyRkQkMZlMwMzerbB+mj8aq5S4nFeMwcsO4KdDV2BGww8iegCWNSIiE+HX3B6xczR4vp0zSssNiNhyCrPXpaCwpEzqaEQkIZY1IiITYm+jwDcTuuEfoe1gIROw40QWBkYl4NS1AqmjEZFEWNaIiEyMTCZg2nMtsXFGAJo2bID0m3cwdHkiViemcyxKZIZY1oiITJRPs0bYGRaMlzq4oLTCgA9jTuONn46h4C7HokTmhGWNiMiENbRWYOV4X3wwoAMs5QJ+PZWNAVHxOJ6ZL3U0InpGWNaIiEycIAiYHOyJn2cEwt2+ATJv3cUrXyXi24Q0jkWJzADLGhFRHeHl3hA7wzQI7dwYZRUiPt5xBlPXJCP/TqnU0YjoKWJZIyKqQ1RKSywb44OPB3eEQi7D72dvoH9kApKv3JY6GhE9JSxrRER1jCAIGB/QHJvfCERzB2tcy7+LEV8fxFf7LsFg4FiUqL5hWSMiqqM6NVVjR5gGg7yaoMIg4tNfz2Hy6iO4VcyxKFF9wrJGRFSH2VpZYMmorlg4tDOsLGTYez4XoUvicTjtltTRiKiWsKwREdVxgiBgdPdm2DozCC2cbJCtK8GolQex9M9UjkWJ6gGWNSKieqK9qwrbZwVjqHdTGETgP79dwMTvDyO3UC91NCJ6AixrRET1iI2VBRaN7IrPX+kCpaUM8al5CI2MR+KlPKmjEVENsawREdVDw7u5Y/usYLRxsUVuoR7jvjmExb9fQAXHokR1DssaEVE91drFDttmBmNkN3cYRGDx76kY980h5OhKpI5GRNXAskZEVI81UMjx71e6YPHIrrBWyHHw8k2ERsYjPjVX6mhE9JhY1oiIzMAQ76bYPjsY7RrbIa+oFBO+O4z/7D6P8gqD1NGI6BFY1oiIzERLJ1tsnRmEsT2aQRSBpXsuYsyqQ8gquCt1NCJ6CJY1IiIzorSU418vd0bUaG/YWlngcPothC6Jx55zOVJHI6IHYFkjIjJDA72aYMfsYHRqqsLtO2V4NfoIFsaeRRnHokQmh2WNiMhMNXe0wS+vB2JSYHMAwNf7L2Pk1wdxLZ9jUSJTwrJGRGTGrCzkmD+oI74a5wM7pQWOZeQjdEk84s7ckDoaEf1/LGtERIS+nVwRG6aBl5saBXfLMHXNUXy84wxKyzkWJZIayxoREQEA3O2tsWlGIKYEewIAvk1Iw/CvEpF5647EyYjMG8saEREZKSxkeG9AB3wzoRvUDSxx/GoBQiPjsetUltTRiMwWyxoREVXxYgcXxM7RwKdZQxSWlGPGj8fw4bZT0JdXSB2NyOywrBER0X01bdgAG6YHYHrPFgCA1QevYNiKRKTnFUucjMi8sKwREdEDWcplmNevPb5/1Q/2NgqcuqbDgKgEbD9+XepoRGaDZY2IiB6pd1tnxIZp0L25PYr05Zi9LgX/2HISJWUcixI9bSxrRET0WBqrlVg7tQdmP98KggCsPZSBIcsO4FJukdTRiOo1ljUiInpsFnIZ/hbSFmsmd4ejrQLnsgsxMCoBW1KuSh2NqN5iWSMiomrTtHZCbJgGAS0ccKe0Am9uOI6//3wcd0s5FiWqbSxrRERUI84qJX6c0gNvvtgGMgHYePQqBi1NQOqNQqmjEdUrLGtERFRjcpmAOS+2xk9T/OFkZ4XUnCIMXJqAjUczIYqi1PGI6gWWNSIiemIBLR3w6xwNNK0dUVJmwN9/PoG/bTyOYn251NGI6jyWNSIiqhWOtlZY/Wp3vN2nLWQCsDnlGgYtTcDZLJ3U0YjqNJY1IiKqNTKZgJm9W2H9tAA0VilxKbcYQ5YdwNpDGRyLEtUQyxoREdW67p72iJ2jQe+2TtCXG/CPLScRtl6LwpIyqaMR1Tksa0RE9FTY2yjw7UQ/zOvXDhYyAduPX8fAqASculYgdTSiOoVljYiInhqZTMD0ni2xYXoAmjZsgPSbdzB0eSLWHEznWJToMbGsERHRU+fr0Qg7w4LxYnsXlFYY8MG205i59hgK7nIsSvQoLGtERPRMNLRWYNUEX3wwoAMs5QJiT2ZjQFQ8jmfmSx2NyKSxrBER0TMjCAImB3vi5xmBcLdvgMxbd/HKV4n4NiGNY1GiB2BZIyKiZ87LvSF2zNagX6fGKKsQ8fGOM5j2QzLy75RKHY3I5LCsERGRJNQNLLF8rA8+GtwRCrkMcWduoH9kAo5l3JY6GpFJYVkjIiLJCIKACQHNsfmNQDR3sMa1/LsY8dVBfL3vEgwGjkWJAJY1IiIyAZ2aqrF9djAGejVBuUHEwl/PYcqao7hVzLEoUY3K2vLly+Hp6QmlUglfX1/Ex8c/cG1WVhbGjBmDtm3bQiaTITw8vMqa6OhoCIJQZSspKTGuWbhwIfz8/GBnZwdnZ2cMGTIE58+fr0l8IiIyQXZKS0SO6opPXu4MKwsZ/jyXg9Al8TiSfkvqaESSqnZZ27BhA8LDwxEREYGUlBRoNBr069cPGRkZ912v1+vh5OSEiIgIeHl5PfC8KpUKWVlZlTalUmm8f9++fZg5cyaSkpIQFxeH8vJyhISEoLi4uLpPgYiITJQgCBjToxm2zgxCCycbZOtKMGplEpbtucixKJktQazmZ6V79OgBHx8frFixwrivffv2GDJkCBYuXPjQY3v16oWuXbti8eLFlfZHR0cjPDwc+fn5j50jNzcXzs7O2LdvH5577rnHOkan00GtVqOgoAAqleqxH4uIiJ69Yn053t96CptTrgEANK0d8eXIrnC0tZI4GT1r5v7+Xa0ra6WlpUhOTkZISEil/SEhIUhMTHyiIEVFRfDw8ICbmxsGDBiAlJSUh64vKLj3t+Xs7e2f6HGJiMg02VhZ4IsRXvjslS5QWsoQn5qH0CXxOHjpptTRiJ6papW1vLw8VFRUwMXFpdJ+FxcXZGdn1zhEu3btEB0djZiYGKxbtw5KpRJBQUFITU2973pRFDF37lwEBwejU6dODzyvXq+HTqertBERUd0hCAJGdHPH9lnBaO1si5xCPcZ+k4TFv19ABceiZCZq9AEDQRAq3RZFscq+6vD398e4cePg5eUFjUaDjRs3ok2bNoiKirrv+lmzZuHEiRNYt27dQ8+7cOFCqNVq4+bu7l7jjEREJJ3WLnaImRWMEd3cYBCBxb+nYvy3h5BTWPLog4nquGqVNUdHR8jl8ipX0XJycqpcbXuiUDIZ/Pz87ntlbfbs2YiJicGePXvg5ub20PPMmzcPBQUFxi0zM7PWMhIR0bPVQCHHZ6944cuRXrBWyJF46SZCl8QjITVP6mhET1W1yppCoYCvry/i4uIq7Y+Li0NgYGCthRJFEVqtFq6urpX2zZo1C5s3b8aff/4JT0/PR57HysoKKpWq0kZERHXby95uiJkVjHaN7ZBXVIrx3x3Cf3afR3mFQepoRE+FRXUPmDt3LsaPH49u3bohICAAK1euREZGBmbMmAHg3tWsa9euYc2aNcZjtFotgHsfIsjNzYVWq4VCoUCHDh0AAAsWLIC/vz9at24NnU6HyMhIaLVaLFu2zHiOmTNnYu3atdi2bRvs7OyMV/fUajUaNGhQ4xeAiIjqnlbOttg6Mwgf7TiDtYcysHTPRRxOv4XIUd5orFY++gREdUi1v7oDuPeluJ999hmysrLQqVMnfPnll8avz5g0aRLS09Oxd+/e/3uQ+/w+m4eHB9LT0wEAb775JjZv3ozs7Gyo1Wp4e3tj/vz5CAgIeOg5AOD777/HpEmTHiu3uX/0l4ioPoo5fh3/2HwSRfpy2NsosGiEF3q1dZY6FtUic3//rlFZq6vM/YdNRFRfpecVY+baYzh9/d6n/mf0bIm/hbSBpZx/VbE+MPf3b/5XTEREdV5zRxv88nogJgZ4AAC+2ncJo1Ym4Vr+XYmTET05ljUiIqoXlJZyLBjcCSvG+sBOaYHkK7fRPzIev5+5IXU0oifCskZERPVKv86uiA3TwMtNjfw7ZZiy5ij+ueMMSsv5aVGqm1jWiIio3nG3t8amGYF4Lfje1zx9k5CG4V8fROatOxInI6o+ljUiIqqXFBYyvD+gA1ZN6AZ1A0scz8xHaGQ8dp3KkjoaUbWwrBERUb32UgcX7AwLhk+zhigsKceMH4/hw22noC+vkDoa0WNhWSMionrPrZE1NkwPwPSeLQAAqw9ewbAViUjPK5Y4GdGjsawREZFZsJTLMK9fe3w/yQ+NrC1x6poOA6ISsOPEdamjET0UyxoREZmV3u2cETtHA7/mjVCkL8estSmI2HISJWUci5JpYlkjIiKz46pugHVT/TGrdysIAvDToQwMWXYAl3KLpI5GVAXLGhERmSULuQxv9WmLNZO7w9FWgXPZhRgYlYCtKdekjkZUCcsaERGZNU1rJ8SGaRDQwgF3SisQvkGLd34+gbulHIuSaWBZIyIis+esUuLHKT0Q/mJrCAKw4WgmBi9LQOqNQqmjEbGsERERAYBcJiD8xTb4aUoPONlZ4cKNIgxaegCbjmZKHY3MHMsaERHR/whs6YjYMA00rR1xt6wCb/98AnM3alGsL5c6GpkpljUiIqK/cLKzwupXu+PtPm0hE4DNx65h0NIEnMvWSR2NzBDLGhER0X3IZAJm9m6F9dMC0FilxKXcYgxeegDrDmdAFEWp45EZYVkjIiJ6iO6e9oido0Gvtk7Qlxswb/NJzFmvRRHHovSMsKwRERE9gr2NAt9N9MO8fu0glwmIOX4dAyLjcepagdTRyAywrBERET0GmUzA9J4tsXF6AJqolUi/eQdDVyTih4PpHIvSU8WyRkREVA2+Ho0QO0eDF9u7oLTcgPe3ncbMtcegKymTOhrVUyxrRERE1dTQWoFVE3zx/oAOsJQLiD2ZjQGRCThxNV/qaFQPsawRERHVgCAIeC3YEz/PCIRbowbIuHUHw1Yk4ruENI5FqVaxrBERET0BL/eG2BmmQd+OjVFWIeKjHWcw/YdkFNzhWJRqB8saERHRE1I3sMSKcT74aHBHKOQy/HbmBkIj43Es47bU0ageYFkjIiKqBYIgYEJAc2x+IxAeDta4ln8XI746iJX7L8Fg4FiUao5ljYiIqBZ1aqrGjtnBGNDFFeUGEZ/EnsOUNUdxu7hU6mhUR7GsERER1TI7pSWiRnvjk5c7Q2Ehw5/nchAaGY8j6bekjkZ1EMsaERHRUyAIAsb0aIZtM4PQwtEGWQUlGLUyCcv2XORYlKqFZY2IiOgpau+qwvbZwXjZuykqDCI+330ek6KPIK9IL3U0qiNY1oiIiJ4yGysLLBrhhc9e6QKlpQz7L+QidEk8ki7flDoa1QEsa0RERM+AIAgY0c0dMbOC0drZFjmFeoxZlYQlv6eigmNRegiWNSIiomeojYsdts0KwnBfNxhE4MvfL2DCd4eQU1gidTQyUSxrREREz5i1wgKfD/fCohFesFbIceDiTYQuSUBCap7U0cgEsawRERFJZKiPG2JmBaNdYzvkFekx/rtD+OK38yivMEgdjUwIyxoREZGEWjnbYuvMIIzu3gyiCET9eRFjvjmE7AKORekeljUiIiKJKS3lWDi0MyJHe8NGIcfhtFsIjYzH3vM5UkcjE8CyRkREZCIGeTXBjjANOjZR4VZxKSZ9fwT/3nUOZRyLmjWWNSIiIhPi6WiDX14PxIQADwDAir2XMGplEq7n35U4GUmFZY2IiMjEKC3l+GhwJ6wY6wM7pQWSr9xGaGQ8/jh7Q+poJAGWNSIiIhPVr7Mrds7WwMtNjfw7ZXht9VH8c8cZlJZzLGpOWNaIiIhMWDMHa2yaEYjJQZ4AgG8S0jDi64PIvHVH4mT0rLCsERERmTiFhQwfDOyAleN9oVJaQJuZj/6R8dh9OlvqaPQMsKwRERHVESEdGyN2jgbezRpCV1KO6T8kY37MaejLK6SORk8RyxoREVEd4tbIGhunB2D6cy0AANGJ6XhlxUFcuVkscTJ6WljWiIiI6hhLuQzzQtvju0nd0MjaEievFWBAZAJ2nsiSOho9BSxrREREddTz7VwQO0cDv+aNUKgvx8y1x/De1pMoKeNYtD5hWSMiIqrDXNUNsG6qP2b2bglBAH5MysDLyxNxObdI6mhUS1jWiIiI6jgLuQxv92mH1a92h4ONAmezdBgYlYBt2mtSR6NawLJGRERUTzzXxgm/ztHAv4U9iksrMGe9Fu/+cgJ3SzkWrctY1oiIiOoRZ5USP03xx5wXWkMQgPVHMjFk2QFczCmUOhrVUI3K2vLly+Hp6QmlUglfX1/Ex8c/cG1WVhbGjBmDtm3bQiaTITw8vMqa6OhoCIJQZSspKTGu2b9/PwYOHIgmTZpAEARs3bq1JtGJiIjqPblMwJsvtcFPr/WAk50Vzt8oxMCoA/g5+arU0agGql3WNmzYgPDwcERERCAlJQUajQb9+vVDRkbGfdfr9Xo4OTkhIiICXl5eDzyvSqVCVlZWpU2pVBrvLy4uhpeXF5YuXVrdyERERGYpsJUjYsM0CG7liLtlFXhr03H8beNx3CktlzoaVYMgiqJYnQN69OgBHx8frFixwrivffv2GDJkCBYuXPjQY3v16oWuXbti8eLFlfZHR0cjPDwc+fn5jxdaELBlyxYMGTKkOtGh0+mgVqtRUFAAlUpVrWOJiIjqKoNBxPK9F7Eo7gIMItDSyQbLx/qibWM7qaM9FnN//67WlbXS0lIkJycjJCSk0v6QkBAkJiY+UZCioiJ4eHjAzc0NAwYMQEpKyhOdD7h3VU+n01XaiIiIzI1MJmDW862xbqo/XFRWuJRbjEFLE7D+cAaqec2GJFCtspaXl4eKigq4uLhU2u/i4oLs7Jr/Mdl27dohOjoaMTExWLduHZRKJYKCgpCamlrjcwLAwoULoVarjZu7u/sTnY+IiKgu69HCAbFhGvRs4wR9uQHvbj6J8A1aFOk5FjVlNfqAgSAIlW6LolhlX3X4+/tj3Lhx8PLygkajwcaNG9GmTRtERUXV+JwAMG/ePBQUFBi3zMzMJzofERFRXedga4XvJ/nh3X7tIJcJ2Ka9joFRCTh9vUDqaPQA1Sprjo6OkMvlVa6i5eTkVLna9kShZDL4+fk98ZU1KysrqFSqShsREZG5k8kEzOjZEhun+6OJWom0vGK8vDwRPyRd4VjUBFWrrCkUCvj6+iIuLq7S/ri4OAQGBtZaKFEUodVq4erqWmvnJCIiosp8PewRO0eDF9s7o7TcgPe3nsKstSnQlZRJHY3+h0V1D5g7dy7Gjx+Pbt26ISAgACtXrkRGRgZmzJgB4N7o8dq1a1izZo3xGK1WC+Dehwhyc3Oh1WqhUCjQoUMHAMCCBQvg7++P1q1bQ6fTITIyElqtFsuWLTOeo6ioCBcvXjTeTktLg1arhb29PZo1a1ajJ09ERGTuGlorsGpCN3ybkIZ/7zqHnSezcPJaAZaO8UYXt4ZSxyPUoKyNHDkSN2/exEcffYSsrCx06tQJsbGx8PDwAHDvS3D/+p1r3t7exn8nJydj7dq18PDwQHp6OgAgPz8f06ZNQ3Z2NtRqNby9vbF//350797deNzRo0fRu3dv4+25c+cCACZOnIjo6OjqPg0iIiL6/wRBwBRNC3Rrbo9Za48h49YdDFuRiH+EtsekwOZP9Hvp9OSq/T1rdZm5f08LERHRoxTcLcM7P5/ArtP3fj+9T0cXfDbMC2prS8kymfv7N/82KBERERmpG1hixTgfLBjUEQq5DLtP30BoZDxSMm5LHc1ssawRERFRJYIgYGJgc/zyeiA8HKxxLf8uhn91EKv2X+anRSXAskZERET31dlNjR2zg9G/iyvKDSL+FXsWU1Yfxe3iUqmjmRWWNSIiInogO6Ullo72xr9e7gSFhQx/nMtBaGQ8jqbfkjqa2WBZIyIioocSBAFje3hg6xtBaOFog6yCEoxcmYTley/CYOBY9GljWSMiIqLH0qGJCttnB+Nl76aoMIj4bNd5vBp9BDeL9FJHq9dY1oiIiOix2VhZYNEIL3w2rAuUljLsu5CL0Mh4JF2+KXW0eotljYiIiKpFEASM8HNHzKxgtHK2xQ2dHmNWJSHyj1RUcCxa61jWiIiIqEbauNghZlYQhvu6wSACi+Iu8ArbU1DtPzdFRERE9F/WCgt8PtwLAS0dcDZLh6BWjlJHqndY1oiIiOiJDfVxkzpCvcUxKBEREZEJY1kjIiIiMmEsa0REREQmjGWNiIiIyISxrBERERGZMJY1IiIiIhPGskZERERkwljWiIiIiEwYyxoRERGRCWNZIyIiIjJhLGtEREREJoxljYiIiMiEsawRERERmTALqQM8S6IoAgB0Op3ESYiIiOhx/fd9+7/v4+bGrMpaYWEhAMDd3V3iJERERFRdhYWFUKvVUsd45gTRjGqqwWDA9evXYWdnB0EQau28Op0O7u7uyMzMhEqlqrXzEhER1RVP871QFEUUFhaiSZMmkMnM7ze4zOrKmkwmg5ub21M7v0qlYlkjIiKz9rTeC83xitp/mV89JSIiIqpDWNaIiIiITBjLWi2wsrLChx9+CCsrK6mjEBERSYLvhU+PWX3AgIiIiKiu4ZU1IiIiIhPGskZERERkwljWiIiIiEwYyxoRERHVC/Pnz4eLiwsEQcDWrVuljlNrWNb+YtKkSRAEwbg5ODigb9++OHHixGMdv2vXLgiCgOzs7Er7GzduXOXPXF29ehWCIOC3336rtfxERFS7cnJyMH36dDRr1gxWVlZo3Lgx+vTpg4MHD9bq4/Tq1Qvh4eG1es6noVevXpXeJ/+6NW/eXJJcZ8+exYIFC/D1118jKysL/fr1kyTH08Cydh99+/ZFVlYWsrKy8Mcff8DCwgIDBgx4rGODg4NhYWGBvXv3GvedPXsWJSUl0Ol0uHjxonH/nj17YGlpiaCgoNp+CkREVEuGDRuG48ePY/Xq1bhw4QJiYmLQq1cv3Lp1S+pokti8ebPxPfLw4cMAgN9//92478iRI5XWl5aWPpNcly5dAgAMHjwYjRs3rvFXiJSVldVmrId67NdGpEomTpwoDh48uNK+/fv3iwDEnJwcURRF8cSJE2Lv3r1FpVIp2tvbi1OnThULCwuN6wMCAsTp06cbby9fvlzs37+/GBoaKq5atcq4f/LkyWJQUNDTfUJERFRjt2/fFgGIe/fufei6/Px8cerUqaKTk5NoZ2cn9u7dW9Rqtcb7P/zwQ9HLy0tcs2aN6OHhIapUKnHkyJGiTqcTRfHeew+ASltaWpooiqJ4+vRpsV+/fqKNjY3o7Owsjhs3TszNzTWeu2fPnuLs2bPFt99+W2zUqJHo4uIifvjhh1Wex9SpU0VnZ2fRyspK7Nixo7h9+3bj/QcOHBA1Go2oVCpFNzc3cfbs2WJRUdEjX5+0tDQRgJiSkmLc5+HhIX788cfixIkTRZVKJU6YMEEURVH8+9//LrZu3Vps0KCB6OnpKb733ntiaWnpY79GoiiKmzZtEjt16mR8/33hhRfEoqIi8cMPP6zy+omiKFZUVIgLFiwQmzZtKioUCtHLy0v89ddfq+TfsGGD2LNnT9HKykr87rvvjF3gX//6l+js7Cyq1Wpx/vz5YllZmfjWW2+JjRo1Eps2bSp+++23lV6Pq1eviiNGjBAbNmwo2tvbi4MGDTL+HP/7cx48eLD4ySefiK6urqKHh8cjX2NRFEVeWXuEoqIi/PTTT2jVqhUcHBxw584d9O3bF40aNcKRI0ewadMm/P7775g1a5bxmN69e2PPnj3G23v27EGvXr3Qs2fPKvt79+79TJ8PERE9PltbW9ja2mLr1q3Q6/X3XSOKIvr374/s7GzExsYiOTkZPj4+eOGFFypdfbt06RK2bt2KHTt2YMeOHdi3bx8+/fRTAMCSJUsQEBCAqVOnGq9Qubu7IysrCz179kTXrl1x9OhR7Nq1Czdu3MCIESMqZVi9ejVsbGxw6NAhfPbZZ/joo48QFxcHADAYDOjXrx8SExPx448/4syZM/j0008hl8sBACdPnkSfPn0wdOhQnDhxAhs2bEBCQkKl97Xq+vzzz9GpUyckJyfj/fffBwDY2dkhOjoaZ86cwZIlS7Bq1Sp8+eWXlY572GuUlZWF0aNHY/LkyTh79iz27t2LoUOHQhRFvPXWW/j++++N67Kysoyv6xdffIH//Oc/OHHiBPr06YNBgwYhNTW10uO+8847CAsLw9mzZ9GnTx8AwJ9//onr169j//79WLRoEebPn48BAwagUaNGOHToEGbMmIEZM2YgMzMTAHDnzh307t0btra22L9/PxISEmBra4u+fftWuoL2xx9/4OzZs4iLi8OOHTse7wV9rEpnRiZOnCjK5XLRxsZGtLGxEQGIrq6uYnJysiiKorhy5UqxUaNGlf6PY+fOnaJMJhOzs7NFURTF3377TQQgXr9+XRRFUXR2dhYPHz4sJiUliU2aNBFFURQzMjJEAOIff/zxjJ8hERFVx88//yw2atRIVCqVYmBgoDhv3jzx+PHjxvv/+OMPUaVSiSUlJZWOa9mypfj111+LonjvqpG1tXWlq0Rvv/222KNHD+Ptnj17inPmzKl0jvfff18MCQmptC8zM1MEIJ4/f954XHBwcKU1fn5+4jvvvCOKoiju3r1blMlkxvV/NX78eHHatGmV9sXHx4symUy8e/fuA18XUXzwlbUhQ4Y89DhRFMXPPvtM9PX1Nd5+1GuUnJwsAhDT09Pve74tW7aIf601TZo0Ef/1r39V2ufn5ye+8cYblfIvXry40pqJEyeKHh4eYkVFhXFf27ZtRY1GY7xdXl4u2tjYiOvWrRNFURS//fZbsW3btqLBYDCu0ev1YoMGDcTdu3cbz+vi4iLq9fpHvDqV8craffTu3RtarRZarRaHDh1CSEgI+vXrhytXruDs2bPw8vKCjY2NcX1QUBAMBgPOnz9vvK1QKLB3716cOXMGd+/ehY+PD3x9faHT6ZCamoo9e/bAysoKgYGBUj1NIiJ6DMOGDcP169cRExODPn36YO/evfDx8UF0dDQAIDk5GUVFRXBwcDBeibO1tUVaWprx96gAoHnz5rCzszPednV1RU5OzkMfOzk5GXv27Kl03nbt2gFApXN36dKl0nH/e26tVgs3Nze0adPmgY8RHR1d6TH69OkDg8GAtLS0x3+h/ke3bt2q7Pv5558RHByMxo0bw9bWFu+//z4yMjIqrXnYa+Tl5YUXXngBnTt3xvDhw7Fq1Srcvn37gRl0Oh2uX79e5ffCg4KCcPbs2Ufm7dixI2Sy/6tJLi4u6Ny5s/G2XC6Hg4ODMV9ycjIuXrwIOzs74+tob2+PkpKSSj+rzp07Q6FQPDD3/VhUa7WZsLGxQatWrYy3fX19oVarsWrVKoiiCEEQ7nvcf/dbW1uje/fu2LNnD27duoXg4GDj5ebAwEDs2bMHBw8eREBAAJRK5dN/QkRE9ESUSiVeeuklvPTSS/jggw8wZcoUfPjhh5g0aRIMBgNcXV0rfbDsvxo2bGj8t6WlZaX7BEGAwWB46OMaDAYMHDgQ//73v6vc5+rq+ljnbtCgwSMfY/r06QgLC6tyX7NmzR567IP87wUNAEhKSsKoUaOwYMEC9OnTB2q1GuvXr8cXX3xRad3DnodcLkdcXBwSExPx22+/ISoqChERETh06BA8PT0fmOWv79n3ex//a94HZXlYPoPBAF9fX/z0009VzuXk5PTQx3oUlrXHIAgCZDIZ7t69iw4dOmD16tUoLi42vuAHDhyATCar9H8tvXv3xvr163H79m306tXLuL9nz57Yu3cvDh48iFdfffVZPxUiIqoFHTp0MH6Pl4+PD7Kzs2FhYfFEX1uhUChQUVFRaZ+Pjw9++eUXNG/eHBYWNXvL7tKlC65evYoLFy7c9+qaj48PTp8+XekiRW07cOAAPDw8EBERYdx35cqVap9HEAQEBQUhKCgIH3zwATw8PLBlyxbMnTu3ylqVSoUmTZogISEBzz33nHF/YmIiunfvXrMn8hA+Pj7YsGEDnJ2doVKpavXcHIPeh16vR3Z2NrKzs3H27FnMnj0bRUVFGDhwIMaOHQulUomJEyfi1KlT2LNnD2bPno3x48fDxcXFeI7evXsjNTUVu3btQs+ePY37e/bsiR07diA9PZ0fLiAiMnE3b97E888/jx9//BEnTpxAWloaNm3ahM8++wyDBw8GALz44osICAjAkCFDsHv3bqSnpyMxMRHvvfcejh49+tiP1bx5cxw6dAjp6enIy8uDwWDAzJkzcevWLYwePRqHDx/G5cuX8dtvv2Hy5MlVit2D9OzZE8899xyGDRuGuLg4pKWl4ddff8WuXbsA3Pvl+oMHD2LmzJnQarVITU1FTEwMZs+eXf0X7AFatWqFjIwMrF+/HpcuXUJkZCS2bNlSrXMcOnQIn3zyCY4ePYqMjAxs3rwZubm5aN++/QOPefvtt/Hvf/8bGzZswPnz5/Huu+9Cq9Vizpw5T/qUqhg7diwcHR0xePBgxMfHIy0tDfv27cOcOXNw9erVJzo3r6zdx65du4yXl+3s7NCuXTts2rTJeIVs9+7dmDNnDvz8/GBtbY1hw4Zh0aJFlc4REBBg/I4XX19f434/Pz9UVFSgQYMG6NGjx7N5QkREVCO2trbo0aMHvvzyS1y6dAllZWVwd3fH1KlT8Y9//APAvas9sbGxiIiIwOTJk5Gbm4vGjRvjueeeq/Q/8Y/y1ltvYeLEiejQoQPu3r2LtLQ0NG/eHAcOHMA777yDPn36QK/Xw8PDA3379q30+1SP8ssvv+Ctt97C6NGjUVxcjFatWhk/ZdmlSxfs27cPERER0Gg0EEURLVu2xMiRI6v3Yj3E4MGD8eabb2LWrFnQ6/Xo378/3n//fcyfP/+xz6FSqbB//34sXrwYOp0OHh4e+OKLLx765bdhYWHQ6XT429/+hpycHHTo0AExMTFo3bp1LTyryqytrbF//3688847GDp0KAoLC9G0aVO88MILT3ylTRBFUaylnERERERUyzgGJSIiIjJhLGtEREREJoxljYiIiMiEsawRERERmTCWNSIiIiITxrJGREREZMJY1oiIiIhMGMsaERERkQljWSMiIiIyYSxrRERERCaMZY2IiIjIhLGsEREREZmw/wfKWw6Xin0YHgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_type = ['BoW','Sentence Transformer']\n",
    "plt.plot(embedding_type,[mse_bow,mse_sentence_transformer])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbed1417-81a6-42e6-86cd-50754265bc3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b03858-7d55-4d40-a7b4-3ab9138b2916",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "790125a7-e07a-4fa2-9b17-6d397648cb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_df['headline A'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "64ae199e-cf8e-494e-ad0b-2e6bd8252e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([104866, 73])\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained DistilBERT tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Example text input\n",
    "text_A = list(final_df['headline A'].values)\n",
    "text_B = list(final_df['headline B'].values)\n",
    "# Tokenize text input\n",
    "tokens = tokenizer(text_A,text_B,padding=True, truncation=True, add_special_tokens=True, return_tensors='pt')\n",
    "print(tokens['input_ids'].shape)\n",
    "\n",
    "input_ids_a = tokens['input_ids'][0]\n",
    "attention_mask_a = tokens['attention_mask'][0]\n",
    "input_ids_b = tokens['input_ids'][1]\n",
    "attention_mask_b = tokens['attention_mask'][1]\n",
    "# print(tokens['input_ids'].shape)\n",
    "# Pass tokens through DistilBERT model\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(tokens['input_ids'], attention_mask=tokens['attention_mask'])\n",
    "    \n",
    "# # Extract the last hidden state activations\n",
    "# last_hidden_state = outputs[0].squeeze().numpy()\n",
    "\n",
    "# # Print the last hidden state activations\n",
    "# print(\"Last Hidden State Activations:\")\n",
    "# print(last_hidden_state.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f9baf0c4-78e0-4f04-a0a9-6edbf9cfc798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "56ba207e-414e-4a5a-b08d-7d469a084de4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104866, 8)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f1a43f4c-b7bb-45e0-bfc2-8fcc63684fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_disbert_vectorizer(df):\n",
    "#     tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "#     model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "    \n",
    "#     text_A = list(final_df['headline A'].values)\n",
    "#     text_B = list(final_df['headline B'].values)\n",
    "#     # Tokenize text input\n",
    "#     tokens = tokenizer(text_A,text_B,padding=True, truncation=True, add_special_tokens=True, return_tensors='pt')\n",
    "#     print(tokens['input_ids'].shape)\n",
    "    \n",
    "#     input_ids_a = tokens['input_ids'][0]\n",
    "#     attention_mask_a = tokens['attention_mask'][0]\n",
    "#     input_ids_b = tokens['input_ids'][1]\n",
    "#     attention_mask_b = tokens['attention_mask'][1]\n",
    "#     # Pass tokens through DistilBERT model\n",
    "#     with torch.no_grad():\n",
    "#         outputs_A = model(input_ids_a, attention_mask=attention_mask_a)\n",
    "#         outputs_B = model(input_ids_b, attention_mask=attention_mask_b)\n",
    "#     # Extract the last hidden state activations\n",
    "#     last_hidden_state_A = outputs_A[0].squeeze().numpy()\n",
    "#     last_hidden_state_B = outputs_B[0].squeeze().numpy()\n",
    "    \n",
    "#     # Print the last hidden state activations\n",
    "#     print(\"Last Hidden State Activations:\")\n",
    "#     print(last_hidden_state_A.shape,last_hidden_state_B.shape)\n",
    "#     return last_hidden_state_A,last_hidden_state_B\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "af563b0e-5c51-485d-9c6b-95f656e08d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_ids_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c6c641db-f054-4bcb-ab6c-bac74e276dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dis_embed_A,dis_embed_B = build_disbert_vectorizer(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9dd0e53b-f656-45b9-8dbb-f12c2a7bedb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 17\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# print(tokens['input_ids'].shape)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# input_ids_a = tokens['input_ids'][0]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# attention_mask_b = tokens['attention_mask'][1]\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Pass tokens through DistilBERT model\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 17\u001b[0m     OP_A \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens_A\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokens_A\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     OP_B \u001b[38;5;241m=\u001b[39m model(tokens_B[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m], attention_mask\u001b[38;5;241m=\u001b[39mtokens_B[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Extract the last hidden state activations\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/proj/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/proj/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:583\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    579\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    581\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(input_ids, inputs_embeds)  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m--> 583\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/proj/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/proj/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:359\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m    357\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_state,)\n\u001b[0;32m--> 359\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    362\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/proj/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/proj/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:313\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    310\u001b[0m sa_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msa_layer_norm(sa_output \u001b[38;5;241m+\u001b[39m x)  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;66;03m# Feed Forward Network\u001b[39;00m\n\u001b[0;32m--> 313\u001b[0m ffn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mffn\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa_output\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m    314\u001b[0m ffn_output: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer_norm(ffn_output \u001b[38;5;241m+\u001b[39m sa_output)  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m    316\u001b[0m output \u001b[38;5;241m=\u001b[39m (ffn_output,)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/proj/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/proj/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:254\u001b[0m, in \u001b[0;36mFFN.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mff_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/proj/lib/python3.9/site-packages/transformers/pytorch_utils.py:248\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/proj/lib/python3.9/site-packages/transformers/models/distilbert/modeling_distilbert.py:257\u001b[0m, in \u001b[0;36mFFN.ff_chunk\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mff_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 257\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlin1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(x)\n\u001b[1;32m    259\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin2(x)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/proj/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/proj/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "text_A = list(final_df['headline A'].values)\n",
    "text_B = list(final_df['headline B'].values)\n",
    "# Tokenize text input\n",
    "tokens_A = tokenizer(text_A,padding=True, truncation=True, add_special_tokens=True, return_tensors='pt')\n",
    "tokens_B = tokenizer(text_B,padding=True, truncation=True, add_special_tokens=True, return_tensors='pt')\n",
    "# print(tokens['input_ids'].shape)\n",
    "\n",
    "# input_ids_a = tokens['input_ids'][0]\n",
    "# attention_mask_a = tokens['attention_mask'][0]\n",
    "# input_ids_b = tokens['input_ids'][1]\n",
    "# attention_mask_b = tokens['attention_mask'][1]\n",
    "# Pass tokens through DistilBERT model\n",
    "with torch.no_grad():\n",
    "    OP_A = model(tokens_A['input_ids'], attention_mask=tokens_A['attention_mask'])\n",
    "    OP_B = model(tokens_B['input_ids'], attention_mask=tokens_B['attention_mask'])\n",
    "# Extract the last hidden state activations\n",
    "last_hidden_state_a = OP_A[:,0].numpy()\n",
    "last_hidden_state_b = OP_B[:,0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01b830ee-cc0a-4c20-ad3a-5295f03e5a5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokens_A' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokens_A\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape,tokens_B[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokens_A' is not defined"
     ]
    }
   ],
   "source": [
    "tokens_A['input_ids'].shape,tokens_B['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c310fb68-a58b-4749-8861-e9671edbf4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 768)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Move model to GPU\n",
    "model = model.to('cuda')\n",
    "\n",
    "text_A = list(final_df['headline A'].values)\n",
    "text_B = list(final_df['headline B'].values)\n",
    "# Tokenize text input\n",
    "tokens_A = tokenizer(text_A,padding=True, truncation=True, add_special_tokens=True, return_tensors='pt')\n",
    "tokens_B = tokenizer(text_B,padding=True, truncation=True, add_special_tokens=True, return_tensors='pt')\n",
    "\n",
    "# Move tokenized inputs to GPU\n",
    "tokens_A = {k:v.to('cuda') for k, v in tokens_A.items()}\n",
    "tokens_B = {k:v.to('cuda') for k, v in tokens_B.items()}\n",
    "\n",
    "# Pass tokens through DistilBERT model\n",
    "with torch.no_grad():\n",
    "    OP_A = model(tokens_A['input_ids'], attention_mask=tokens_A['attention_mask'])\n",
    "    OP_B = model(tokens_B['input_ids'], attention_mask=tokens_B['attention_mask'])\n",
    "\n",
    "# Move output tensors back to CPU and convert to numpy arrays\n",
    "last_hidden_state_a = OP_A[:,0].cpu().numpy()\n",
    "last_hidden_state_b = OP_B[:,0].cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb4acb6-1554-4de0-bf57-84477f932d43",
   "metadata": {},
   "source": [
    "In some cases, the first token (often referred to as the \"[CLS]\" token) may contain important contextual information that summarizes the entire input text, especially when fine-tuned on a specific task. This is because in models like DistilBERT, the \"[CLS]\" token is used as a special token to represent the entire sequence during pre-training, and during fine-tuning it can be used to capture task-specific information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b90fce98-9b29-40e6-b69a-268727c57302",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m     OP_A \u001b[38;5;241m=\u001b[39m model(tokens_A[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m], attention_mask\u001b[38;5;241m=\u001b[39mtokens_A[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# OP_B = model(tokens_B['input_ids'], attention_mask=tokens_B['attention_mask'])\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Extract the last hidden state activations\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m last_hidden_state_a \u001b[38;5;241m=\u001b[39m \u001b[43mOP_A\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# last_hidden_state_b = OP_B[:,0].numpy()\u001b[39;00m\n\u001b[1;32m     23\u001b[0m last_hidden_state_a\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/proj/lib/python3.9/site-packages/transformers/utils/generic.py:288\u001b[0m, in \u001b[0;36mModelOutput.__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_dict[k]\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# text_A = list(final_df['headline A'].values)\n",
    "# text_B = list(final_df['headline B'].values)\n",
    "text_A = \"This is test text to check how thte shape comes out\"\n",
    "# Tokenize text input\n",
    "tokens_A = tokenizer(text_A,padding=True, truncation=True, add_special_tokens=True, return_tensors='pt')\n",
    "# tokens_B = tokenizer(text_B,padding=True, truncation=True, add_special_tokens=True, return_tensors='pt')\n",
    "# print(tokens['input_ids'].shape)\n",
    "\n",
    "# input_ids_a = tokens['input_ids'][0]\n",
    "# attention_mask_a = tokens['attention_mask'][0]\n",
    "# input_ids_b = tokens['input_ids'][1]\n",
    "# attention_mask_b = tokens['attention_mask'][1]\n",
    "# Pass tokens through DistilBERT model\n",
    "with torch.no_grad():\n",
    "    OP_A = model(tokens_A['input_ids'], attention_mask=tokens_A['attention_mask']).last_hidden_state\n",
    "    # OP_B = model(tokens_B['input_ids'], attention_mask=tokens_B['attention_mask'])\n",
    "# Extract the last hidden state activations\n",
    "last_hidden_state_a = OP_A[:,0].numpy()\n",
    "# last_hidden_state_b = OP_B[:,0].numpy()\n",
    "last_hidden_state_a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6350d272-25d2-4be3-859b-8299cc0241a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
